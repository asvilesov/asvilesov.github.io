<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Alexander 'Sasha' Vilesov</title>
  
  <meta name="author" content="Alexander 'Sasha' Vilesov">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <link rel="stylesheet" href="stylesheet.css">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"> -->
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Alexander 'Sasha' Vilesov</name>
              </p>
              <p>I am a computer vision PhD student in the <a href="https://visual.ee.ucla.edu/">Visual Machines Group</a> with Prof. Achuta Kadambi at <a href="https://www.ucla.edu/">UCLA</a>. 
              </p>
              <p>
                Before starting my PhD, I received my Bachelor's in Electrical and Computer Engineering at the <a href="https://www.usc.edu/">University of Southern California</a> (USC) in 2021 and worked in the GSP Lab with Prof. Antonio Ortega and the S2L2 Lab on inverse reinforcement learning with Prof. Rahul Jain. In 2020-2021, I worked at <a href="https://www.jpl.nasa.gov/">NASA JPL</a> on TriG GPS receivers (mounted on <a href="https://www.nesdis.noaa.gov/COSMIC-2">Cosmic-2</a>) to support tracking of <a href="https://en.wikipedia.org/wiki/Galileo_(satellite_navigation)">GALILEO</a> satellites. In 2025, I interned at a startup, Terrawise AI, developing computer vision models for robot navigation and solar asset monitoring as well as agentic LLM applications for robotic fleet management. 
              </p>
              <p style="text-align:center">
                <a class="icon-link" href="https://scholar.google.com/citations?user=LtHRlX4AAAAJ&hl=en" target="_blank" title="Google Scholar">
                  <img src="images/googlescholar.webp" alt="Google Scholar" class="icon-img">
                </a>
                <a class="icon-link" href="https://www.linkedin.com/in/alexander-vilesov-3b524013b" target="_blank" title="LinkedIn">
                  <img src="images/linkedin.png" alt="LinkedIn" class="icon-img">
                </a>
                <!-- <a class="icon-link" href="https://twitter.com/SashaVilesov" target="_blank" title="X / Twitter">
                  <img src="images/twitter X.png" alt="Twitter" class="icon-img">
                </a>
                <a class="icon-link" href="https://github.com/asvilesov" title="Github">
                  <img src="images/github.png" alt="Github" class="icon-img">
                </a> -->
                <a class="icon-link" href="data/Vilesov_Resume.pdf" target="_blank" title="CV">
                  <img src="images/resume_cv.png" alt="CV" class="icon-img">
                </a>
              </p>
              <p style="text-align:center">
                vilesov@ucla.edu &nbsp/&nbsp
                (626)-390-7241
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/sasha_portrait.jpg"><img style="width:100%;max-width:100%;" alt="profile photo" src="images/sasha_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My current research focuses on equipping and evaluating vision-language models (VLMs) with spatial intelligence — understanding and reasoning about 3D/4D, physical relationships, and reasoning overfrom multimodal input. I am also broadly interested in topics related to large language models, diffusion models, digital health, and camera security.
              </p>
              <heading>Papers</heading>
              <p>* indicates equal contribution</p>
              <!-- <heading>Current Research</heading>
              <p>
                Non-contact Biomedical devices and algorithms have the ambitious goals of accurately extracting vital signs and/or diagnosing a subject with an underlying condition. It is obvious why they are so attractive, they promise to quickly, accurately, and in a non-invasive way assess the health condition of a person. But, there are barriers to these goals. One is the susceptibility to algorithmic error due to variance in a population, we can see this as the problem of generalization. Secondly, devices/algorithms may have an inherent bias to certain parts of the population, we see this clearly in RPPG and thermal loading in IRT devices. Lastly, devices often target one physical or health modality. Combining various modalities into a composite sensor can expand its capabilities. We will study non-contact biomedical devices and algorithms to analyze and overcome these barriers. The result should be two-fold, to identify bias in current devices and algorithms through understanding their mechanics and conducting empirical tests; then, a solution should reduce these shortcomings. If we succeed, then we will pave the way for a more efficient and fairer healthcare. 
              </p> -->
            </td>
          </tr>
        
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="equipleth_611_stop()" onmouseover="equipleth_611_start()"  bgcolor="#ffffff">
            <td style="padding:20px;width:40%;vertical-align:middle">
              <div class="one">
                <div class="two" id='equipleth_611'><video  width=100% height=100% muted autoplay loop>
                <source src="images/vlm4d.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vlm4d.png' width="325">
              </div>
              <script type="text/javascript">
                function equipleth_611_start() {
                  document.getElementById('equipleth_img_611').style.opacity = "1";
                }

                function equipleth_611_stop() {
                  document.getElementById('equipleth_img_611').style.opacity = "0";
                }
                equipleth_611_stop()
              </script>
            </td>
            <td style="padding:0px;width:100%;vertical-align:middle">
              
                <papertitle>VLM4D: Towards Spatiotemporal Awareness in Vision Language Models</papertitle>

              <br>
              <a>Shijie Zhou*</a>,
              <strong>Alexander Vilesov*</strong>,
              <a>Xuehai He</a>,
              <a>Ziyu Wan</a>,
              <a>Shuwang Zhang</a>,
              <a>Aditya Nagachandra</a>,
              <a>Di Chang</a>,
              <a>Dongdong Chen</a>,
              <a>Xin Eric Wang</a>,
              <a href="https://www.ee.ucla.edu/achuta-kadambi/">Achuta Kadambi</a>,
              <br>
              <em>ICCV
                ​</em>, 2025 &nbsp 
              <br>
              <a href="https://vlm4d.github.io/">Project Page</a> /
              <a href="https://www.arxiv.org/pdf/2508.02095">Paper Link</a>
              <p></p>
              <p> We introduce a benchmark to evaluate vision-language models on spatiotemporal reasoning in real-world and synthetic videos, revealing limited intelligence in problems that are exceedingly easy for humans. We show this through a detailed analysis of our dataset across a large set of today's top model and present preliminary methods for improving spatiotemporal intelligence. </p>
            </td>
          </tr> 
        
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="equipleth_611_stop()" onmouseover="equipleth_611_start()"  bgcolor="#ffffff">
            <td style="padding:20px;width:40%;vertical-align:middle">
              <div class="one">
                <div class="two" id='equipleth_611'><video  width=100% height=100% muted autoplay loop>
                <source src="images/chimera_camera_display.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/chimera_camera_display.png' width="325">
              </div>
              <script type="text/javascript">
                function equipleth_611_start() {
                  document.getElementById('equipleth_img_611').style.opacity = "1";
                }

                function equipleth_611_stop() {
                  document.getElementById('equipleth_img_611').style.opacity = "0";
                }
                equipleth_611_stop()
              </script>
            </td>
            <td style="padding:0px;width:100%;vertical-align:middle">
              
                <papertitle>Chimera: Creating Digitally Signed Fake Photos by Fooling Image Recapture and Deepfake Detectors</papertitle>

              <br>
              <a>Seongbin Park*</a>,
              <strong>Alexander Vilesov*</strong>,
              <a>Jinghuai Zhang</a>,
              <a>Hossein Khalili</a>,
              <a>Yuan Tian</a>,
              <a href="https://www.ee.ucla.edu/achuta-kadambi/">Achuta Kadambi</a>,
              <a>Nader Sehatbakhsh</a>,
              <br>
              <em>to appear in 34th Usenix Security Symposium (Usenix Security)
                ​</em>, 2025 &nbsp 
              <br>
              <a href="https://drive.google.com/file/d/1vMMBk2twidJnaakmOgR-63bzBdi1wByf/view?usp=sharing">Paper Link</a> /
              <a href="https://github.com/ssysarch/Chimera.git">Code</a>
              <p></p>
              <p> We present Chimera, an end-to-end attack that creates cryptographically signed (e.g. C2PA) fake images capable of fooling both deepfake and image recapture detectors. The key idea is developing a two-step attack strategy where the transformation caused by image recapture can be learned and hence compensated for by a model that in turn regenerates detection-free images.</p>
            </td>
          </tr> 
        
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="equipleth_611_stop()" onmouseover="equipleth_611_start()"  bgcolor="#ffffff">
            <td style="padding:20px;width:40%;vertical-align:middle">
              <div class="one">
                <div class="two" id='equipleth_611'><video  width=100% height=100% muted autoplay loop>
                <source src="images/teaser_implicitppg.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/teaser_implicitppg.png' width="325">
              </div>
              <script type="text/javascript">
                function equipleth_611_start() {
                  document.getElementById('equipleth_img_611').style.opacity = "1";
                }

                function equipleth_611_stop() {
                  document.getElementById('equipleth_img_611').style.opacity = "0";
                }
                equipleth_611_stop()
              </script>
            </td>
            <td style="padding:0px;width:100%;vertical-align:middle">
              
                <papertitle>Implicit Neural Models to Extract Heart Rate from Video</papertitle>

              <br>
              <a href="https://pradyumnachari.github.io/">Pradyumna Chari</a>,
              <a href="https://anirudh0707.github.io">Anirudh B. Harish</a>,
              <a href="https://adnan-armouti.github.io">Adnan Armouti</a>,
              <strong>Alexander Vilesov</strong>,
              <a> Sanjit Sarda</a>
              <a href="https://www.uclahealth.org/providers/laleh-jalilian">Laleh Jalilian</a>,
              <a href="https://www.ee.ucla.edu/achuta-kadambi/">Achuta Kadambi</a>
              <br>
              <em>ECCV</em>, 2024 &nbsp 
              <br>
              <a href="https://implicitppg.github.io/">Project Page</a> /
              <a href="https://drive.google.com/file/d/1yxeVvcLhECcG7hPWVHr0tSyBIfLlUr9N/view">Paper Link</a> /
              <a href="https://github.com/UCLA-VMG/FastImplicitPleth">Code</a>
              <p></p>
              <p> We propose a new implicit neural representation, that enables fast and accurate decomposition of face videos into blood and appearance components. This allows contactless estimation of heart rate from challenging out-of-distribution face videos. </p>
            </td>
          </tr> 
        
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="equipleth_611_stop()" onmouseover="equipleth_611_start()"  bgcolor="#ffffff">
            <td style="padding:20px;width:40%;vertical-align:middle">
              <div class="one">
                <div class="two" id='equipleth_611'><video  width=100% height=100% muted autoplay loop>
                <source src="images/image_deepfakes.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/image_deepfakes.png' width="325">
              </div>
              <script type="text/javascript">
                function equipleth_611_start() {
                  document.getElementById('equipleth_img_611').style.opacity = "1";
                }

                function equipleth_611_stop() {
                  document.getElementById('equipleth_img_611').style.opacity = "0";
                }
                equipleth_611_stop()
              </script>
            </td>
            <td style="padding:0px;width:100%;vertical-align:middle">
              
                <papertitle>Solutions to Deepfakes: Can Camera Hardware, Cryptography, and Deep Learning Verify Real Images?</papertitle>

              <br>
              <strong>Alexander Vilesov</strong>,
              <a href="https://www.ytian.info/">Yuan Tian</a>,
              <a href="https://ssysarch.ee.ucla.edu/nader/">Nader Sehatbakhsh</a>,
              <a href="https://www.ee.ucla.edu/achuta-kadambi/">Achuta Kadambi</a>
              <br>
              <em>White Paper</em>, 2024 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2407.04169">Paper Link</a>
              <p></p>
              <p> The rapid advancement of generative AI threatens the credibility of real images and videos, as synthetic content will soon be indistinguishable from camera-captured media and easily accessible to all. This white paper explores detection and cryptographic methods to reliably differentiate real images from synthetic ones, analyzing existing strategies and proposing improvements to enhance their effectiveness. </p>
            </td>
          </tr> 
        
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="equipleth_611_stop()" onmouseover="equipleth_611_start()"  bgcolor="#ffffff">
            <td style="padding:20px;width:40%;vertical-align:middle">
              <div class="one">
                <div class="two" id='equipleth_611'><video  width=100% height=100% muted autoplay loop>
                <source src="images/cg3d_demo.gif" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/cg3d_demo.gif' width="325">
              </div>
              <script type="text/javascript">
                function equipleth_611_start() {
                  document.getElementById('equipleth_img_611').style.opacity = "1";
                }

                function equipleth_611_stop() {
                  document.getElementById('equipleth_img_611').style.opacity = "0";
                }
                equipleth_611_stop()
              </script>
            </td>
            <td style="padding:0px;width:100%;vertical-align:middle">
              
                <papertitle>CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting</papertitle>

              <br>
              <strong>Alexander Vilesov*</strong>,
              <a href="https://pradyumnachari.github.io/">Pradyumna Chari*</a>,
              <a href="https://www.ee.ucla.edu/achuta-kadambi/">Achuta Kadambi</a>
              <br>
              <em>Arxiv</em>, 2023 &nbsp 
              <br>
              <a href="https://asvilesov.github.io/CG3D/">Project Page</a> /
              <a href="https://arxiv.org/abs/2311.17907">Paper Link</a>
              <p></p>
              <p> We present a method for 3D generation of multi-object realistic scenes from text by utilizing text-to-image diffusion models and Gaussian radiance fields. These scenes are decomposable and editable at the object level. </p>
            </td>
          </tr> 



        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="equipleth_611_stop()" onmouseover="equipleth_611_start()"  bgcolor="#ffffff">
            <td style="padding:20px;width:40%;vertical-align:middle">
              <div class="one">
                <div class="two" id='equipleth_611'><video  width=100% height=100% muted autoplay loop>
                <source src="images/irt.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/irt.png' width="325">
              </div>
              <script type="text/javascript">
                function equipleth_611_start() {
                  document.getElementById('equipleth_img_611').style.opacity = "1";
                }

                function equipleth_611_stop() {
                  document.getElementById('equipleth_img_611').style.opacity = "0";
                }
                equipleth_611_stop()
              </script>
            </td>
            <td style="padding:0px;width:100%;vertical-align:middle">
              
                <papertitle>Making thermal imaging more equitable and accurate: resolving solar loading biases</papertitle>

              <br>
              <a href="https://ellinz.com/">Ellin Zhao</a>,
              <strong>Alexander Vilesov</strong>,
              Shreeram Athreya,
              <a href="https://pradyumnachari.github.io/">Pradyumna Chari</a>,
              Jeanette Merlos,
              Kendall Millett, 
              Nia St Cyr,
              <a href="https://www.uclahealth.org/providers/laleh-jalilian">Laleh Jalilian</a>,
              <a href="https://www.ee.ucla.edu/achuta-kadambi/">Achuta Kadambi</a>
              <br>
              <em>Arxiv</em>, 2023 &nbsp 
              <br>
              <a href="https://arxiv.org/abs/2304.08832">Paper Link</a>
              <p></p>
              <p> Despite the wide use of thermal sensors for temperatures screening, estimates from thermal sensors do not work well in uncontrolled scene conditions such as after sun exposure. We propose a single-shot correction scheme to eliminate solar loading bias in the time of a typical frame exposure (33ms). </p>
            </td>
          </tr> 





        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="equipleth_611_stop()" onmouseover="equipleth_611_start()"  bgcolor="#ffffff">
            <td style="padding:20px;width:40%;vertical-align:middle">
              <div class="one">
                <div class="two" id='equipleth_611'><video  width=100% height=100% muted autoplay loop>
                <source src="images/siggraph_611_represent.jpg" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/siggraph_611_represent.jpg' width="325">
              </div>
              <script type="text/javascript">
                function equipleth_611_start() {
                  document.getElementById('equipleth_img_611').style.opacity = "1";
                }

                function equipleth_611_stop() {
                  document.getElementById('equipleth_img_611').style.opacity = "0";
                }
                equipleth_611_stop()
              </script>
            </td>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <a href="https://visual.ee.ucla.edu/equi_pleth_camera_rf.htm/">
                <papertitle>Blending Camera and 77 GHz Radar Sensing for Equitable, Robust Plethysmography</papertitle>
              </a>
              <br>
              <strong>Alexander Vilesov*</strong>,
              <a href="https://pradyumnachari.github.io/">Pradyumna Chari*</a>,
              <a href="https://adnan-armouti.github.io">Adnan Armouti*</a>,
              <a href="https://anirudh0707.github.io">Anirudh B. Harish</a>,
              Kimaya Kulkarni,
              Ananya Deoghare,
              <a href="https://www.uclahealth.org/providers/laleh-jalilian">Laleh Jalilian</a>,
              <a href="https://www.ee.ucla.edu/achuta-kadambi/">Achuta Kadambi</a>
              <br>
							<em>SIGGRAPH</em>, 2022 &nbsp 
              <br>
              <a href="https://visual.ee.ucla.edu/equi_pleth_camera_rf.htm/">Project Page</a>
              /
              <a href="https://dl.acm.org/doi/10.1145/3528223.3530161">Paper Link</a>
              /
              <a href="https://github.com/UCLA-VMG/EquiPleth">Code and Dataset</a>
              <p></p>
              <p>To overcome fundamental skin-tone biases in camera-based remote plethysmography, we propose an adversarial learning-based fair fusion method, using a novel RGB Camera and FMCW Radar hardware setup. </p>
            </td>
          </tr> 
        
  
          

        </tbody></table>

        
        
</body>

</html>
